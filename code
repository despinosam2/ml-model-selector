# Define the questions
questions = [
    "Is the relationship between variables likely linear?",
    "Do I need interpretability (easy to explain to stakeholders)?",
    "Is the data likely nonlinear or have complex interactions?",
    "Is the dataset large (in terms of samples)?",
    "Is the dataset small or medium-sized?",
    "Are there missing values or outliers in the data?",
    "Do I need to handle multicollinearity (highly correlated features)?",
    "How fast does the model need to train or predict?",
    "Is uncertainty estimation important in predictions?",
    "Do I plan to automatically retrain or deploy this model in production?"
]

# Define the models and their profiles (scores from 0 to 10 on each question)
models = {
    "Linear Regression": [10, 10, 1, 9, 6, 4, 6, 9, 1, 9],
    "Ridge/Lasso Regression": [9, 8, 2, 9, 7, 5, 9, 8, 1, 9],
    "Decision Tree Regressor": [3, 8, 7, 6, 9, 9, 5, 7, 2, 7],
    "Random Forest": [2, 4, 9, 9, 7, 9, 6, 7, 2, 7],
    "Gradient Boosting (XGBoost)": [2, 4, 9, 9, 6, 8, 7, 6, 3, 6],
    "Support Vector Regression (SVR)": [3, 5, 8, 6, 6, 4, 6, 5, 1, 5],
    "Gaussian Process Regression (GPR)": [2, 9, 8, 2, 10, 6, 5, 4, 10, 5],
    "Neural Networks (MLP)": [1, 3, 10, 10, 4, 6, 5, 5, 3, 4]
}

# Create sliders for each question
sliders = [widgets.IntSlider(value=5, min=0, max=10, description=f"Q{i+1}", continuous_update=False) for i in range(len(questions))]

# Function to calculate and display model rankings
def evaluate_model(**kwargs):
    input_scores = list(kwargs.values())
    results = {}
    for model, profile in models.items():
        # Calculate a similarity score by taking the minimum of the user's input and the model's profile for each question.
        # A higher score indicates a better match between the data characteristics and the model's strengths.
        score = sum(min(i, p) for i, p in zip(input_scores, profile))
        results[model] = score
    ranked = sorted(results.items(), key=lambda x: x[1], reverse=True)

    display(Markdown("### üß† Top Suggested Models:"))
    for model, score in ranked[:3]:
        display(Markdown(f"**{model}** ‚Äî Total Score: {score}"))

    display(Markdown("#### üìä Full Scores"))
    for model, score in ranked:
        display(Markdown(f"- {model}: {score}"))

# Create interactive UI
interactive_ui = widgets.interactive(evaluate_model, **{f"Q{i+1}": sliders[i] for i in range(len(questions))})

# Display the instructions and the interactive UI
display(Markdown("## üßê Choosing the Right Regression Model"))
display(Markdown("This interactive tool helps you select a suitable regression model based on the characteristics of your data and project requirements. For each question below, adjust the slider to indicate the relevance of that factor (0 being not relevant at all, and 10 being highly relevant)."))
for i, q in enumerate(questions):
    display(Markdown(f"**Q{i+1}:** {q}"))
display(interactive_ui)
